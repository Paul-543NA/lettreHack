{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from base64 import b64encode\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from anthropic import Anthropic\n",
    "from requests import get\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dir = \"./PREM Historical records/\"\n",
    "\n",
    "files = [\n",
    "    f\n",
    "    for f in [\n",
    "        root + \"/\" + f\n",
    "        for root, dirs, files in os.walk(_dir)\n",
    "        for f in [f for f in files if f.endswith((\".pdf\", \".PDF\"))]\n",
    "    ]\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    os.mkdir(i.removesuffix(\".pdf\"))\n",
    "    # ghostscript installation required\n",
    "    results = subprocess.run(\n",
    "        [\n",
    "            \"gs\",\n",
    "            \"-dNOPAUSE\",\n",
    "            \"-sDEVICE=jpeg\",\n",
    "            \"-r250\",\n",
    "            f'-sOutputFile={file.removesuffix(\".pdf\")}/-%02d.jpg',\n",
    "            f\"{file}\",\n",
    "            \"-dBATCH\",\n",
    "        ],\n",
    "        stdout=subprocess.PIPE,\n",
    "    )\n",
    "    print(results.stdout.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Anthropic(\n",
    "    api_key=\"\"\n",
    ")\n",
    "image_media_type = \"image/jpeg\"\n",
    "\n",
    "\n",
    "# https://evidence-hou.se/events/big-llm-hack-24/data/correspondence.html\n",
    "content = get(\n",
    "    \"https://evidence-house-public.s3.eu-west-2.amazonaws.com/national_archive_records.zip\"\n",
    ").content\n",
    "\n",
    "with ZipFile(BytesIO(content), \"r\") as f:\n",
    "    f.extractall(\"./\")\n",
    "os.rename(\" PREM Historical records\", \"PREM Historical records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    {\n",
    "        \"title\": \"Date\",\n",
    "        \"tag\": \"date\",\n",
    "        \"description\": \"The date the letter was written in YYYY-MM-DD format, or N/A if unknown. If the year and month are known but not the day, use the first day of the month.\",\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Sender\",\n",
    "        \"tag\": \"sender\",\n",
    "        \"description\": \"The person or department which sent the letter, or N/A if unknown\",\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Recipient\",\n",
    "        \"tag\": \"recipient\",\n",
    "        \"description\": \"The person or department which received the letter, or N/A if unknown\",\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Subject\",\n",
    "        \"tag\": \"subject\",\n",
    "        \"description\": \"A one-line subject of the letter if present, otherwise infer this yourself from the context\",\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Document type\",\n",
    "        \"tag\": \"document-type\",\n",
    "        \"description\": \"State the category type of the document (letter, meeting minutes, balance sheet etc)\",\n",
    "    },\n",
    "    # forgot to add bool for handwritten or not\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_string = \"\\n\".join(\n",
    "    f\"- {field['title']}: {field['description']}. Use the key name '{field['tag']}' .\"\n",
    "    for field in fields\n",
    ")\n",
    "prompt = f\"\"\"\n",
    "\n",
    "Transcribe the text in this image in full, in json format, with the key \"text\".\n",
    "\n",
    "Please also extract the following fields:\n",
    "\n",
    "{fields_string}\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = []\n",
    "for file in files:\n",
    "    with open(file, \"rb\") as f:\n",
    "        image_data.append(base64.b64encode(f.read()).decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, image in tqdm(zip(files, image_data)):\n",
    "    if os.path.exists(file.rstrip(\".jpg\") + \".txt\"):\n",
    "        print(\"skip\")\n",
    "        continue\n",
    "    print(\"api\")\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": image_media_type,\n",
    "                            \"data\": image,\n",
    "                        },\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    parsed_doc = message.content[0].text\n",
    "    with open(file.rstrip(\".jpg\") + \".txt\", \"w\") as f:\n",
    "        f.write(parsed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    f\n",
    "    for f in [\n",
    "        root + \"/\" + f\n",
    "        for root, dirs, files in os.walk(\"../\")\n",
    "        for f in [f for f in files if f.endswith((\".jpg\"))]\n",
    "    ]\n",
    "]\n",
    "\n",
    "json_list = []\n",
    "for idx, file in enumerate(files):\n",
    "    with open(file.removesuffix(\".jpg\") + \".txt\", \"r\") as f:\n",
    "        data = f.read()\n",
    "        try:\n",
    "            if not data.startswith(\"{\"):\n",
    "                data = data[data.index(\"{\") :]\n",
    "        except ValueError as e:\n",
    "            data = '{\\n    \"text\": \"\",\\n    \"date\": \"N/A\",\\n    \"sender\": \"N/A\",\\n    \"recipient\": \"N/A\",\\n    \"subject\": \"N/A\",\\n    \"document-type\": \"N/A\"\\n}'\n",
    "        if not data.endswith(\"{\"):\n",
    "            data = data[: data.rfind(\"}\") + 1]\n",
    "        try:\n",
    "            data.index(\"}\")\n",
    "        except ValueError as ve:\n",
    "            print(f\"{file} is not complete\")\n",
    "            continue\n",
    "        try:\n",
    "            data.index('\"date\":')\n",
    "        except ValueError as ve:\n",
    "            print(f\"{file} is not complete\")\n",
    "            continue\n",
    "        if data.index(\"}\") < data.index('\"date\":'):\n",
    "            data = (\n",
    "                data[: data.index(\"}\")]\n",
    "                + \",\"\n",
    "                + data[data[data.find(\"{\") + 1 :].find(\"{\") + 2 :]\n",
    "            )\n",
    "        try:\n",
    "            json_f = json.loads(data)\n",
    "            json_f[\"source\"] = file.removeprefix(\"../PREM Historical records/\")\n",
    "            json_list.append(json_f)\n",
    "        except ValueError as e:\n",
    "            print(file.removesuffix(\".jpg\") + \".txt\" + \"could not process file\")\n",
    "            continue\n",
    "\n",
    "    # missing delimeters (\",\") after a kv pair is also an issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(json_list).replace(\"N/A\", np.nan)\n",
    "# df[\"source\"] = df[\"source\"].apply(lambda x: \"/\".join(x.split(\"/\")[:-1]) + \"/\" + x.split(\"/\")[-1].zfill(8))\n",
    "# df = df.sort_values([\"source\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to rows that have both a sender and recipient\n",
    "subset = df.dropna(subset=[\"sender\", \"recipient\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to rows that have both a sender and recipient\n",
    "subset = df.dropna(subset=[\"sender\", \"recipient\"])\n",
    "# filter to rows identified as letters\n",
    "subset = subset[subset[\"document-type\"].isin([\"Letter\", \"letter\"])]\n",
    "subset.to_csv(\"single-page-letters.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
